{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Julia - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Getting Started with Optimization.jl](#getting_started)\n",
    "- [Equality and Inequality Constraints](#equality_and_inequality_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Optimization.jl <a id=\"getting_started\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we introduce the basics of Optimization.jl by showing how to easily mix local optimizers and global optimizers on the Rosenbrock equation. The simplest copy-pasteable code using a quasi-Newton method (LBFGS) to solve the Rosenbrock problem is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999997057368228\n",
       " 0.999999398151528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Optimization, Zygote\n",
    "\n",
    "rosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\n",
    "u0 = zeros(2)\n",
    "p = [1.0, 100.0]\n",
    "\n",
    "optf = OptimizationFunction(rosenbrock, AutoZygote())\n",
    "prob = OptimizationProblem(optf, u0, p)\n",
    "\n",
    "sol = solve(prob, Optimization.LBFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a different solver package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OptimizationOptimJL is a wrapper for [Optim.jl](#https://github.com/JuliaNLSolvers/Optim.jl) and OptimizationBBO is a wrapper for [BlackBoxOptim.jl](#https://github.com/robertfeldt/BlackBoxOptim.jl).\n",
    "\n",
    "First let's use the `NelderMead`, a derivative free solver from Optim.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999634355313174\n",
       " 0.9999315506115275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using OptimizationOptimJL\n",
    "\n",
    "sol = solve(prob, Optim.NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlackBoxOptim.jl offers derivative-free global optimization solvers that requrie the bounds to be set via `lb` and `ub` in the `OptimizationProblem`. Let's use the `BBOadaptivederand1binradiuslimited()` solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: MaxIters\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999999999999996\n",
       " 0.999999999999999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using OptimizationBBO\n",
    "\n",
    "prob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\n",
    "sol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution from the original solver can always be obtained via `original`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlackBoxOptim.OptimizationResults(\"adaptive_de_rand_1_bin_radiuslimited\", \"Max number of steps (10000) reached\", 10001, 1.744326347439e9, 0.25999999046325684, BlackBoxOptim.ParamsDictChain[BlackBoxOptim.ParamsDictChain[Dict{Symbol, Any}(:RngSeed => 417099, :SearchRange => [(-1.0, 1.0), (-1.0, 1.0)], :TraceMode => :silent, :Method => :adaptive_de_rand_1_bin_radiuslimited, :MaxSteps => 10000),Dict{Symbol, Any}()],Dict{Symbol, Any}(:CallbackInterval => -1.0, :TargetFitness => nothing, :TraceMode => :compact, :FitnessScheme => BlackBoxOptim.ScalarFitnessScheme{true}(), :MinDeltaFitnessTolerance => 1.0e-50, :NumDimensions => :NotSpecified, :FitnessTolerance => 1.0e-8, :TraceInterval => 0.5, :MaxStepsWithoutProgress => 10000, :MaxSteps => 10000…)], 10129, BlackBoxOptim.ScalarFitnessScheme{true}(), BlackBoxOptim.TopListArchiveOutput{Float64, Vector{Float64}}(1.4298103907130839e-30, [0.9999999999999996, 0.999999999999999]), BlackBoxOptim.PopulationOptimizerOutput{BlackBoxOptim.FitPopulation{Float64}}(BlackBoxOptim.FitPopulation{Float64}([0.9999999999999993 0.9999999999999967 … 0.9999999999999987 0.9999999999999951; 0.9999999999999986 0.9999999999999938 … 0.9999999999999967 0.9999999999999898], NaN, [1.67632942359465e-30, 3.0814879110195774e-29, 1.4298103907130839e-30, 4.1464501330679433e-29, 1.1290571705975731e-29, 9.502076122419969e-29, 3.215840783940031e-29, 1.1517369216226772e-28, 3.6780639705929675e-29, 1.1088426099012847e-28  …  7.30416981487878e-24, 6.536768206580833e-24, 6.4902670748893506e-24, 1.5767236055740796e-24, 7.622368496698027e-29, 2.012211605895784e-28, 7.389284751108506e-28, 2.4603832076744714e-28, 4.614836295542919e-29, 4.35845650134609e-29], 0, BlackBoxOptim.Candidate{Float64}[BlackBoxOptim.Candidate{Float64}([0.9999999999999987, 0.9999999999999967], 49, 4.614836295542919e-29, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [1.0, 1.0, 1.0, 0.5225441649232383, 0.823592110936846, 0.9492396213404222, 0.950483568442687, 0.6546379110542061, 0.6510993018154945, 1.0  …  0.1529469345690056, 0.7486547030267282, 1.0, 0.9744748124528216, 0.994860322813759, 0.5966125820335233, 0.9217332489316061, 1.0, 0.5915653647031562, 0.5435754594403481], [1.0, 1.0, 0.09232341098198213, 0.8726707250672715, 0.11633528577074574, 0.7726433219479181, 0.8942504016671119, 0.27664676097311447, 0.10286532579824743, 0.07097043954409472  …  0.21517252038102985, 0.9168835503596723, 1.0, 0.16421214413007124, 1.0, 0.8861062854622073, 0.020683723638279833, 0.9731073551119833, 0.5421031219803052, 0.5392833073121162])), 0), BlackBoxOptim.Candidate{Float64}([0.9999999999999978, 0.9999999999999946], 49, 1.0477058897466563e-28, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [1.0, 1.0, 1.0, 0.5225441649232383, 0.823592110936846, 0.9492396213404222, 0.950483568442687, 0.6546379110542061, 0.6510993018154945, 1.0  …  0.1529469345690056, 0.7486547030267282, 1.0, 0.9744748124528216, 0.994860322813759, 0.5966125820335233, 0.9217332489316061, 1.0, 0.5915653647031562, 0.5435754594403481], [1.0, 1.0, 0.09232341098198213, 0.8726707250672715, 0.11633528577074574, 0.7726433219479181, 0.8942504016671119, 0.27664676097311447, 0.10286532579824743, 0.07097043954409472  …  0.21517252038102985, 0.9168835503596723, 1.0, 0.16421214413007124, 1.0, 0.8861062854622073, 0.020683723638279833, 0.9731073551119833, 0.5421031219803052, 0.5392833073121162])), 0)], Base.Threads.SpinLock(0))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sol.original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization.jl assumes that your objective function takes two arguments `objective(x, p)`\n",
    "\n",
    "1. The optimization variables `x`.\n",
    "2. Other parameters `p`, such as hyper parameters of the cost function. If you have no “other parameters”, you can safely disregard this argument. \n",
    "\n",
    "> Note: If your objective function is defined by someone else, you can create an anonymous function that just discards the extra parameters like this:\n",
    "\n",
    "```julia\n",
    "obj = (x, p) -> objective(x) # Pass this function into OptimizationFunction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Gradient Calculations (Automatic Differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both of the above methods were derivative-free methods, and thus no gradients were required to do the optimization. However, often first order optimization (i.e., using gradients) is much more efficient. Defining gradients can be done in two ways. One way is to manually provide a gradient definition in the `OptimizationFunction` constructor. However, the more convenient way to obtain gradients is to provide an AD backend type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's now use the OptimizationOptimJL `BFGS` method to solve the same problem. We will import the forward-mode automatic differentiation library (using ForwardDiff) and then specify in the `OptimizationFunction` to automatically construct the derivative functions using ForwardDiff.jl. This looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999999999373614\n",
       " 0.999999999868622"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "optf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\n",
    "prob = OptimizationProblem(optf, u0, p)\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the original to see the statistics on the number of steps required and gradients computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     7.645684e-21\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 3.48e-07 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 3.48e-07 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 6.91e-14 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 9.03e+06 ≰ 0.0e+00\n",
       "    |g(x)|                 = 2.32e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    16\n",
       "    f(x) calls:    53\n",
       "    ∇f(x) calls:   53\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sol.original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, it's a lot less than the derivative-free methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the compute cost of forward-mode automatic differentiation scales via the number of inputs, and thus as our optimization problem grows large it slows down. To counteract this, for larger optimization problems (>100 state variables) one normally would want to use reverse-mode automatic differentiation. One common choice for reverse-mode automatic differentiation is Zygote.jl. We can demonstrate this via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999999999373614\n",
       " 0.999999999868622"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Zygote\n",
    "\n",
    "optf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())\n",
    "prob = OptimizationProblem(optf, u0, p)\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Box Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, one knows the potential bounds on the solution values. In Optimization.jl, these can be supplied as the `lb` and `ub` arguments for the lower bounds and upper bounds respectively, supplying a vector of values with one per state variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999999993561103\n",
       " 0.9999999987161009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob = OptimizationProblem(optf, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 1.0000000007498584\n",
       " 1.0000000015022157"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob = OptimizationProblem(optf, u0, p, lb = [-1.0, -1.0], ub = [Inf, Inf])\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 3×3 Matrix{Float64}:\n",
       " -1.0  -1.0  -1.0\n",
       " -1.0  -1.0  -1.0\n",
       " -1.0  -1.0  -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = [0.5 0.5 0.5\n",
    "      0.5 0.5 0.5\n",
    "      0.5 0.5 0.5]\n",
    "A = [1. 1. 1.\n",
    "     1. 1. 1.\n",
    "     1. 1. 1.]\n",
    "opt_fun(x, p) = sum(A*x)\n",
    "p = 1\n",
    "optf = OptimizationFunction(opt_fun, Optimization.AutoZygote())\n",
    "prob = OptimizationProblem(optf, x0, p, lb = [-1.0 -1.0 -1.0; -1.0 -1.0 -1.0; -1.0 -1.0 -1.0], ub = [1.0 1.0 1.0; 1.0 1.0 1.0; 1.0 1.0 1.0])\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equality and Inequality Constraints <a id=\"equality_and_inequality_constraints\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
