{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Optimization.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999997057368228\n",
       " 0.999999398151528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the package and define the problem to optimize\n",
    "using Optimization, Zygote\n",
    "rosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\n",
    "u0 = zeros(2)\n",
    "p = [1.0, 100.0]\n",
    "\n",
    "optf = OptimizationFunction(rosenbrock, AutoZygote())\n",
    "prob = OptimizationProblem(optf, u0, p)\n",
    "\n",
    "sol = solve(prob, Optimization.LBFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a different solver package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OptimizationOptimJL is a wrapper for [Optim.jl](#https://github.com/JuliaNLSolvers/Optim.jl) and OptimizationBBO is a wrapper for [BlackBoxOptim.jl](#https://github.com/robertfeldt/BlackBoxOptim.jl).\n",
    "\n",
    "First let's use the NelderMead a derivative free solver from Optim.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: Success\n",
       "u: 2-element Vector{Float64}:\n",
       " 0.9999634355313174\n",
       " 0.9999315506115275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using OptimizationOptimJL\n",
    "sol = solve(prob, Optim.NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlackBoxOptim.jl offers derivative-free global optimization solvers that requrie the bounds to be set via `lb` and `ub` in the `OptimizationProblem`. Let's use the _BBOadaptivederand1binradiuslimited()_ solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retcode: MaxIters\n",
       "u: 2-element Vector{Float64}:\n",
       " 1.0\n",
       " 0.9999999999999993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using OptimizationBBO\n",
    "prob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\n",
    "sol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution from the original solver can always be obtained via `original`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlackBoxOptim.OptimizationResults(\"adaptive_de_rand_1_bin_radiuslimited\", \"Max number of steps (10000) reached\", 10001, 1.738093894447e9, 0.1510000228881836, BlackBoxOptim.ParamsDictChain[BlackBoxOptim.ParamsDictChain[Dict{Symbol, Any}(:RngSeed => 2798, :SearchRange => [(-1.0, 1.0), (-1.0, 1.0)], :TraceMode => :silent, :Method => :adaptive_de_rand_1_bin_radiuslimited, :MaxSteps => 10000),Dict{Symbol, Any}()],Dict{Symbol, Any}(:CallbackInterval => -1.0, :TargetFitness => nothing, :TraceMode => :compact, :FitnessScheme => BlackBoxOptim.ScalarFitnessScheme{true}(), :MinDeltaFitnessTolerance => 1.0e-50, :NumDimensions => :NotSpecified, :FitnessTolerance => 1.0e-8, :TraceInterval => 0.5, :MaxStepsWithoutProgress => 10000, :MaxSteps => 10000…)], 10103, BlackBoxOptim.ScalarFitnessScheme{true}(), BlackBoxOptim.TopListArchiveOutput{Float64, Vector{Float64}}(4.4373425918681914e-29, [1.0, 0.9999999999999993]), BlackBoxOptim.PopulationOptimizerOutput{BlackBoxOptim.FitPopulation{Float64}}(BlackBoxOptim.FitPopulation{Float64}([0.9999999999882851 0.9999999999999415 … 1.0 0.9999999999957705; 0.9999999999761017 0.9999999999999365 … 0.9999999999999993 0.9999999999914782], NaN, [1.5919869330607833e-22, 2.8978471320004314e-25, 4.020769799724263e-26, 3.9277835403569125e-23, 1.871498541926986e-24, 1.5638101300493153e-23, 2.4462730745299127e-25, 1.2794678002818662e-25, 3.8673905878460104e-25, 2.417210452705439e-25  …  1.3598298002538293e-26, 8.711352765905537e-26, 2.4216565516533552e-23, 8.066102755884846e-29, 2.4303512730127074e-23, 1.2429662501964804e-22, 1.2431706218703103e-22, 2.92866495701307e-24, 4.4373425918681914e-29, 1.828358716970366e-23], 0, BlackBoxOptim.Candidate{Float64}[BlackBoxOptim.Candidate{Float64}([0.9999999999998563, 0.9999999999997267], 3, 4.020769799724263e-26, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [1.0, 0.8112149565862794, 1.0, 0.6412391608276748, 0.9876409796724125, 0.4546485119189785, 0.52000050812248, 0.6630629987682936, 0.7546550975702044, 0.22689585584435912  …  1.0, 0.654265504073408, 1.0, 1.0, 0.8573696710220029, 0.8387497328198233, 0.974626220386457, 1.0, 0.4961618060410592, 0.6772413744082779], [0.9137078400995814, 0.08263978503669815, 0.025613216807354552, 0.8427202960120496, 1.0, 0.204096752404021, 0.10441078143962657, 1.0, 0.15847785147073767, 0.49826291962963104  …  0.19383106289984067, 0.22662237428545676, 0.180652831374405, 0.9962865866518722, 0.8195098006347892, 0.14540948865461545, 0.10592914469944964, 1.0, 0.33051802617807413, 1.0])), 0), BlackBoxOptim.Candidate{Float64}([0.9999999999999417, 0.9999999999997708], 3, 1.2707467580853732e-24, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [1.0, 0.8112149565862794, 1.0, 0.6412391608276748, 0.9876409796724125, 0.4546485119189785, 0.52000050812248, 0.6630629987682936, 0.7546550975702044, 0.22689585584435912  …  1.0, 0.654265504073408, 1.0, 1.0, 0.8573696710220029, 0.8387497328198233, 0.974626220386457, 1.0, 0.4961618060410592, 0.6772413744082779], [0.9137078400995814, 0.08263978503669815, 0.025613216807354552, 0.8427202960120496, 1.0, 0.204096752404021, 0.10441078143962657, 1.0, 0.15847785147073767, 0.49826291962963104  …  0.19383106289984067, 0.22662237428545676, 0.180652831374405, 0.9962865866518722, 0.8195098006347892, 0.14540948865461545, 0.10592914469944964, 1.0, 0.33051802617807413, 1.0])), 0)], Base.Threads.SpinLock(0))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sol.original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization.jl assumes that your objective function takes two arguments `objective(x, p)`\n",
    "\n",
    "1. The optimization variables `x`.\n",
    "2. Other parameters `p`, such as hyper parameters of the cost function. If you have no “other parameters”, you can safely disregard this argument. \n",
    "\n",
    "> Note: If your objective function is defined by someone else, you can create an anonymous function that just discards the extra parameters like this:\n",
    "\n",
    "```julia\n",
    "obj = (x, p) -> objective(x) # Pass this function into OptimizationFunction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Gradient Calculations (Automatic Differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both of the above methods were derivative-free methods, and thus no gradients were required to do the optimization. However, often first order optimization (i.e., using gradients) is much more efficient. Defining gradients can be done in two ways. One way is to manually provide a gradient definition in the `OptimizationFunction` constructor. However, the more convenient way to obtain gradients is to provide an AD backend type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's now use the OptimizationOptimJL `BFGS` method to solve the same problem. We will import the forward-mode automatic differentiation library (using ForwardDiff) and then specify in the `OptimizationFunction` to automatically construct the derivative functions using ForwardDiff.jl. This looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
